{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class EssayDisectionModel(nn.Module):\n",
    "    def __init__(self, num_labels=7, hidden_size=768):\n",
    "        super(EssayDisectionModel, self).__init__()\n",
    "        \n",
    "        self.backbone = AutoModel.from_pretrained(\n",
    "        'allenai/longformer-base-4096',\n",
    "        num_labels=num_labels\n",
    "        )\n",
    "        \n",
    "        self.dense1 = nn.Linear(hidden_size, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(256, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        backbone_output = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        x = self.dense1(backbone_output[0])\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: [6, 6, 6, 6, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Lead_1': ['Sample', 'text', 'from', 'an', 'essay...']}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "labels = ['Claim', 'Evidence', 'Concluding Statement', 'Rebuttal', 'Position','Counterclaim', 'Lead']\n",
    "\n",
    "model = torch.load('../data/model.pth')\n",
    "model.to('mps')\n",
    "model.eval()\n",
    "\n",
    "def pad_sequence(sequence, max_len, padding_value=0):\n",
    "    if len(sequence) < max_len:\n",
    "        return sequence + [padding_value] * (max_len - len(sequence))\n",
    "    else:\n",
    "        return sequence[:max_len]\n",
    "\n",
    "\n",
    "def predict(text):\n",
    "\n",
    "    encoded = tokenizer(text, max_length=205, padding='max_length')\n",
    "    padded_input = encoded['input_ids']\n",
    "    mask = encoded['attention_mask']\n",
    "\n",
    "    padded_input_tensor = torch.tensor(padded_input, dtype=torch.long).to('mps')\n",
    "    mask_tensor = torch.tensor(mask, dtype=torch.long).to('mps')\n",
    "\n",
    "    padded_input_batch = padded_input_tensor.unsqueeze(0)\n",
    "    mask_batch = mask_tensor.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(padded_input_batch, mask_batch)\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    relevant_predictions = predictions[0][mask_tensor.bool()].cpu().numpy()\n",
    "\n",
    "    token_to_word = encoded.words()[:len(relevant_predictions)]\n",
    "\n",
    "    word_predictions = []\n",
    "\n",
    "    for idx, label in zip(token_to_word, relevant_predictions):\n",
    "        if idx and idx >= len(word_predictions):\n",
    "            word_predictions.append(label)\n",
    "\n",
    "    return word_predictions\n",
    "\n",
    "\n",
    "def visualize(text, predictions):\n",
    "    words = text.split()\n",
    "    tags = [labels[i] for i in predictions]\n",
    "\n",
    "    tag_dict = {}\n",
    "    current_tag = None\n",
    "    sequence_number = {}\n",
    "\n",
    "    for word, tag in zip(words, tags):\n",
    "        # If the tag changes, reset the current tag and increment sequence number\n",
    "        if tag != current_tag:\n",
    "            current_tag = tag\n",
    "            sequence_number[tag] = sequence_number.get(tag, 0) + 1\n",
    "            key = f\"{tag}_{sequence_number[tag]}\"\n",
    "            tag_dict[key] = []\n",
    "\n",
    "        # Add the word to the current sequence\n",
    "        key = f\"{tag}_{sequence_number[tag]}\"\n",
    "        tag_dict[key].append(word)\n",
    "\n",
    "    return tag_dict\n",
    "\n",
    "def inference(text):\n",
    "    predictions = predict(text)\n",
    "    tag_dict = visualize(text, predictions)\n",
    "    return tag_dict\n",
    "\n",
    "text = \"Sample text from an essay...\"\n",
    "prediction = predict(text)\n",
    "print(\"Predicted label:\", prediction)\n",
    "visualize(text, prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lead_1 :\n",
      "During a group project, have you ever asked a group member about adding or replacing something? Or, when you were studying for a math test, did you ever ask your parents or sibling about different ways to tackle a certain\n",
      "\n",
      "Position_1 :\n",
      "problem? Asking for other's opinions is especially\n",
      "\n",
      "Claim_1 :\n",
      "beneficial as it allows for an individual to receive a variety of different views towards a given topic. Likewise, being diverse and asking many people for their opinions allows one to understand how most people percieve something. This is especially important as knowing multiple opinions can allow\n",
      "\n",
      "Evidence_1 :\n",
      "someone to take those views into account and sway themseleves to the general audience. Knowing different people's\n",
      "\n",
      "Claim_2 :\n",
      "opinion can\n",
      "\n",
      "Position_2 :\n",
      "be beneficial in a variety of situations. First and foremost, a great example about how knowing other's\n",
      "\n",
      "Concluding Statement_1 :\n",
      "opinions is helpful is\n",
      "\n",
      "Evidence_2 :\n",
      "when someone is making the choice between smoking or\n",
      "\n",
      "Concluding Statement_2 :\n",
      "refraining from smoking.\n",
      "\n",
      "Evidence_3 :\n",
      "A student can watch on a TV channel that smoking is bad, and can damage their internal organs. However, on another channel, the student can find advertisements about the most addicting smoking device that can release the most dopomine in the brain, all the while not severly harming people's lungs. This student will receive a variety of different views and opinions on a certain topic, which allows them to make the best educated\n",
      "\n",
      "Concluding Statement_3 :\n",
      "choice or decision based on how they interpret what they saw. Similarily, a student can be told from his fellow classmates that smoking is fun, joyful, and makes them happy. However, if the student asks a\n",
      "\n",
      "Evidence_4 :\n",
      "local doctor, they will be informed differently. A doctor will most likely tell them that smoking, although seeming harmless at first, can lead to serious long term consequences. If the student asks both his friends and his doctors, he is able to use his judgemental skills to determing which choice will be best for him in the long run. Furthermore, asking for\n",
      "\n",
      "Concluding Statement_4 :\n",
      "multiple opinions can\n",
      "\n",
      "Evidence_5 :\n",
      "benifit\n",
      "\n",
      "Concluding Statement_5 :\n",
      "during competitions for a position slot, as cadidates needs to make decisions on what they need to say or do. For example, it can be helpful in situations like elections, both for\n",
      "\n",
      "Position_3 :\n",
      "the U.S. or simply in school. If a student is running for a position in office to represent his/her school, he/she can ask a widespread and diverse audience. First, asking other students is their best bet to obtaining information. Other students can inform him/her about what they want, like better water fountains,\n",
      "\n",
      "Claim_3 :\n",
      "recess, or healthier food. Then, the student running can make changes to the way they run for the election, and on his/her speech, take a different approach. In\n",
      "\n",
      "Evidence_6 :\n",
      "addition, if the student running asks an adult, they will get to know a more realistic way the school can be improved. Since a student, even as a student officer, isn't able to make a significant change to a school, they can inform the school board about ways\n",
      "\n",
      "Position_4 :\n",
      "to make the school better. If someone is running for the president of the United States, a similar approach can be taken. First, they can ask the people, on social media or in\n",
      "\n",
      "Claim_4 :\n",
      "speeches,\n",
      "\n",
      "Position_5 :\n",
      "about positive ways\n",
      "\n",
      "Claim_5 :\n",
      "to reform our country. After the candidate receives the\n",
      "\n",
      "Evidence_7 :\n",
      "opinion of general audiences, they can campaign differently to match the view of those voting. All in all, asking for the opinion of multiple different people can set the candidate apart from others. Many people only ask one type of audience for their opinion. Having only one opinion can lead to negative consequences, such as making the wrong choices related to health or education, as only one audience is adressed into making a decision. Therefore, asking multiple different people who have different backgrounds is essential to making the best choices in life. Conclusively, knowing multiple opinions on a certain matter\n",
      "\n",
      "Concluding Statement_6 :\n",
      "can evidently lead to better results for individuals.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruvchandwani/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:360: FutureWarning: `BatchEncoding.words()` property is deprecated and should be replaced with the identical, but more self-explanatory `BatchEncoding.word_ids()` property.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def read_text_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "file_path = '../data/feedback-prize-2021/test/0FB0700DAF44.txt'\n",
    "\n",
    "text = read_text_from_file(file_path)\n",
    "\n",
    "result = inference(text)\n",
    "\n",
    "for key, value in result.items():\n",
    "    print(key, ':')\n",
    "    print(' '.join(value))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 47241, 2788, 31, 41, 14700, 734, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096', use_fast=True)\n",
    "\n",
    "tokenized_input = tokenizer(\"Sample text from an essay...\", max_length=205, padding='max_length')\n",
    "\n",
    "print(tokenized_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
